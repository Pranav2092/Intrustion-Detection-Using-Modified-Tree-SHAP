!pip install shap

import numpy as np
import shap
import random
from sklearn.ensemble import RandomForestClassifier
from collections import defaultdict

def compute_node_importance(tree, feature_names):

    node_importance = tree.tree_.compute_feature_importances(normalize=False)
    return {feature_names[i]: node_importance[i] for i in range(len(feature_names))}


def detect_feature_interactions(data, model):

    interactions = {}
    corr_matrix = np.corrcoef(data.T)
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix)):
            if np.abs(corr_matrix[i, j]) > 0.5:
                interactions[(i, j)] = corr_matrix[i, j]
    return interactions


def calculate_node_contribution(tree, node):
    return tree.tree_.impurity[node] * tree.tree_.n_node_samples[node]

def initialize_data_structures(data, feature_names):

    shap_values = np.zeros((data.shape[0], len(feature_names)))
    return shap_values

def calculate_shap_contribution(node, feature_contributions):
    contribution = feature_contributions[node]
    return contribution

def adjust_for_interactions(contribution, feature_interactions, feature_names):
    adjustment = np.zeros(len(feature_names))
    for (f1, f2), interaction_value in feature_interactions.items():
        adjustment[feature_names.index(f1)] += contribution * interaction_value
        adjustment[feature_names.index(f2)] += contribution * interaction_value
    return adjustment

def depth_first_traversal(tree, depth_threshold):
    def traverse(node, depth):
        if depth <= depth_threshold:
            yield node
            if tree.tree_.children_left[node] != -1:
                yield from traverse(tree.tree_.children_left[node], depth + 1)
            if tree.tree_.children_right[node] != -1:
                yield from traverse(tree.tree_.children_right[node], depth + 1)
    yield from traverse(0, 0)

def compute_shap_values(tree, node):
    return np.array([tree.tree_.impurity[node]] * tree.tree_.n_node_samples[node])

def detailed_shap_computation(tree, node):
    return tree.tree_.impurity[node] * tree.tree_.n_node_samples[node]

def approximate_shap(tree, node):
    return tree.tree_.impurity[node] * 0.5

def select_feature_subset(feature_names, subset_size, sampling_method):
    num_features = int(len(feature_names) * subset_size)
    if sampling_method == 'importance':
        return random.sample(feature_names, num_features)
    elif sampling_method == 'stratified':
        return feature_names[:num_features]

def compute_subset_shap(subset_features, model, data):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(data[subset_features])
    return shap_values

def advanced_sampling(data, sampling_method, feature_importance):
    if sampling_method == 'importance':
        return data.sample(frac=0.5, weights=feature_importance, axis=1)
    elif sampling_method == 'stratified':
        return data.sample(frac=0.5, random_state=1)

def compute_shap_for_sampled_data(sampled_data, model):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(sampled_data)
    return shap_values

def combine_and_refine(shap_values, feature_interactions):
    refined_values = shap_values.copy()
    for (f1, f2), interaction_value in feature_interactions.items():
        refined_values[:, f1] += refined_values[:, f2] * interaction_value
        refined_values[:, f2] += refined_values[:, f1] * interaction_value
    return refined_values

def modified_tree_shap(model, data, feature_names, depth_threshold=0.1, subset_size=0.5, sampling_method='importance'):
    shap_values = np.zeros((data.shape[0], data.shape[1]))

    # Node Importance and Feature Interactions
    node_importance = {}
    for i, tree in enumerate(model.estimators_):
        node_importance[i] = compute_node_importance(tree)

    feature_interactions = detect_feature_interactions(data, feature_names)

    # Dynamic Feature Interaction Handling
    for i, tree in enumerate(model.estimators_):
        for node in range(tree.tree_.node_count):
            if node in node_importance[i]:
                contribution = calculate_node_contribution(tree, node)
                shap_values += adjust_for_interactions(contribution, feature_interactions, feature_names)

    # Adaptive Tree Depth Limiting
    for i, tree in enumerate(model.estimators_):
        for node in depth_first_traversal(tree, depth_threshold):
            if node_importance[i][node] > depth_threshold:
                shap_values += compute_shap_values(tree, node)

    # Hierarchical Importance-Based Caching
    cache = {}
    for i, tree in enumerate(model.estimators_):
        for node in range(tree.tree_.node_count):
            if node_importance[i][node] > depth_threshold:
                cache[node] = detailed_shap_computation(tree, node)
            else:
                cache[node] = approximate_shap(tree, node)

    # Approximate SHAP with Feature Subsets
    subset_features = select_feature_subset(feature_names, subset_size, sampling_method)
    shap_values += compute_subset_shap(subset_features, model, data)

    # Advanced Sampling Techniques
    sampled_data = advanced_sampling(data, sampling_method, feature_importance=model.feature_importances_)
    shap_values += compute_shap_for_sampled_data(sampled_data, model)

    # Combine and Refine Results
    shap_values = combine_and_refine(shap_values, feature_interactions)

    return shap_values

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/UNSW_NB15_training-set.csv'
unsw_nb15_data = pd.read_csv(file_path)

print(unsw_nb15_data.head())
print(unsw_nb15_data.dtypes)

categorical_cols = unsw_nb15_data.select_dtypes(include=['object']).columns.tolist()
print("Categorical columns:", categorical_cols)

unsw_nb15_data_encoded = pd.get_dummies(unsw_nb15_data, columns=categorical_cols, drop_first=True)

X = unsw_nb15_data_encoded.iloc[:, :-1]
y = unsw_nb15_data_encoded.iloc[:, -1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)


y_pred = rf_model.predict(X_test)


accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the model: {accuracy * 100:.2f}%")

explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test)

print(shap_values)

shap.summary_plot(shap_values, X_test, max_display=20)
